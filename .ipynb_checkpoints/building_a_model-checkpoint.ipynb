{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL READING\n",
    "\n",
    "Hello! You've opened a notebook that's **OPTIONAL**. You don't have to read it. That's because it's **optional.** You can close it if you want, or you can read it if you want. Either one is fine. But please don't ask in OH or on Ed if you need to do anything with this notebook. You don't. Because it's **optional.** But hopefully you find it interesting :) \n",
    "\n",
    "## Walkthrough of Building a Model\n",
    "\n",
    "For this **optional** reading, we will take you on a brief walkthrough of how to create models of your data that can be used to understand and make predictions about your dataset. The following cells have code that is provided for you. You can't run some of them because they rely on completed versions of the code from the main notebook. Copy that other stuff over if you want. Otherwise, you can investigate the pre-computed outputs as you follow along. There is also a discussion of the model's underlying features that you are welcome to read.\n",
    "\n",
    "\n",
    "### What Is My Kind of Song?\n",
    "\n",
    "Our aim will be to build a model that is capable of recognizing whether or not some **new** song would fit into my library or not. In order to do that, it will be important to have examples of songs that are not currently among my Liked Songs. These come from an anonymous friend—let's call her \"E\"—and are found in the file called `other_songs.csv`.\n",
    "\n",
    "The main hypothesis that drives the model I'm building is that the songs that I have saved to my library might tend to fit a certain profile. Maybe my taste is, for example, happy, high-energy, acoustic-sounding music. By introducing songs that come from another library—from another person with a different taste for music—we might introduce some examples that have notably different musical profiles. This will give our model a notion of what music outside of my personal taste might look like so that it can learn to identify examples of it.\n",
    "\n",
    "The first step for building our model will be to add a new column to the underlying data. We'll call this column `\"harry\"` and have it represent whether or not a song came from my library. Songs in my library will take the value `1` for this attribute and songs from E's library will take the value `0`. Then, we'll join the two libraries together using `pd.concat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist Name(s)</th>\n",
       "      <th>harry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>Denial</td>\n",
       "      <td>Mannequin Pussy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>Breaks</td>\n",
       "      <td>The Black Keys</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>10 Minutes 10 Years</td>\n",
       "      <td>Tennis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>Freddy My Love - From \"Grease Live!\" Music Fro...</td>\n",
       "      <td>Keke Palmer, Kether Donohue, Vanessa Hudgens, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>Effect and Cause</td>\n",
       "      <td>The White Stripes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Friend of Nothing - Acoustic</td>\n",
       "      <td>Together Pangea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>Down Rodeo</td>\n",
       "      <td>Rage Against The Machine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>Schubert: Auf dem Wasser zu singen, Op. 72, D....</td>\n",
       "      <td>Franz Schubert, Barbara Bonney, Geoffrey Parsons</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Olivia</td>\n",
       "      <td>One Direction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Submarine</td>\n",
       "      <td>Silicon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>Accommodate</td>\n",
       "      <td>Frankie Cosmos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>Viva La Vida</td>\n",
       "      <td>Coldplay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>Baby You're Out</td>\n",
       "      <td>Mac DeMarco</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>No Surprises</td>\n",
       "      <td>Juliana Chahayed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>Future Starts Slow</td>\n",
       "      <td>The Kills</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Track Name   \n",
       "1519                                             Denial  \\\n",
       "2494                                             Breaks   \n",
       "1374                                10 Minutes 10 Years   \n",
       "939   Freddy My Love - From \"Grease Live!\" Music Fro...   \n",
       "2633                                   Effect and Cause   \n",
       "125                        Friend of Nothing - Acoustic   \n",
       "4075                                         Down Rodeo   \n",
       "745   Schubert: Auf dem Wasser zu singen, Op. 72, D....   \n",
       "249                                              Olivia   \n",
       "1988                                          Submarine   \n",
       "1015                                        Accommodate   \n",
       "2668                                       Viva La Vida   \n",
       "1316                                    Baby You're Out   \n",
       "84                                         No Surprises   \n",
       "2297                                 Future Starts Slow   \n",
       "\n",
       "                                         Artist Name(s)  harry  \n",
       "1519                                    Mannequin Pussy      1  \n",
       "2494                                     The Black Keys      1  \n",
       "1374                                             Tennis      1  \n",
       "939   Keke Palmer, Kether Donohue, Vanessa Hudgens, ...      0  \n",
       "2633                                  The White Stripes      1  \n",
       "125                                     Together Pangea      1  \n",
       "4075                           Rage Against The Machine      1  \n",
       "745    Franz Schubert, Barbara Bonney, Geoffrey Parsons      0  \n",
       "249                                       One Direction      0  \n",
       "1988                                            Silicon      1  \n",
       "1015                                     Frankie Cosmos      1  \n",
       "2668                                           Coldplay      1  \n",
       "1316                                        Mac DeMarco      1  \n",
       "84                                     Juliana Chahayed      0  \n",
       "2297                                          The Kills      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "harry = read_songs('sharry_songs.csv')\n",
    "harry[\"harry\"] = 1\n",
    "not_harry = read_songs('other_songs.csv')\n",
    "not_harry[\"harry\"] = 0\n",
    "\n",
    "combined = pd.concat([harry, not_harry])\n",
    "combined.sample(15)[['Track Name', 'Artist Name(s)', 'harry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model for Taste\n",
    "\n",
    "We can use this combined DataFrame as the source for a **logistic regression model** implemented in the `statsmodels` library that we import below. A logistic regression model is a specific example of a **regression**, which is a statistical technique that builds a mathematical description for how different features of your data influence some outcome. In building a logistic regression model, we are constructing a mathematical object that explains how different features of the songs can predict whether a song is \"one of mine\" or \"one of E's\". \n",
    "\n",
    "Whenever we build a logistic regression model, we have to let `statsmodels` know which column we think can be predicted using a combination of the others. \n",
    "\n",
    "```python\n",
    "model = smf.logit(\"harry ~ Energy + Danceability + Speechiness + Acousticness + Instrumentalness + Liveness + Valence\", data=combined)\n",
    "```\n",
    "\n",
    "The big, long string passed in as an argument to `smf.logit()` tells `statsmodels` that we would like to see how well the `harry` value of a song can be predicted by a combination of the `Energy`, `Danceability`, `Speechiness`, `Acousticness`, `Instrumentalness`, `Liveness`, and `Valence` of that song. \n",
    "\n",
    "When we specify the model, all we have to do is pick the features that we expect to be useful in predicting which person's library the song belongs to. We don't provide any explicit information about which way each feature goes, though; instead, we ask Python to **fit** the model to the data we provided it. This is the (complex) process of taking each song in the dataset, looking at whether it's a \"Harry song\" or an \"E song\", and then learning how much each of the specified features actually matter for songs of either category. After the model is fitted to the data, we can inspect it to determine the influence of each feature. `result.summary()` returns a table with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.447949\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>harry</td>      <th>  No. Observations:  </th>   <td>  5835</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  5827</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 02 Aug 2024</td> <th>  Pseudo R-squ.:     </th>   <td>0.1712</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>14:45:47</td>     <th>  Log-Likelihood:    </th>  <td> -2613.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -3153.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.789e-229</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>    2.1849</td> <td>    0.213</td> <td>   10.273</td> <td> 0.000</td> <td>    1.768</td> <td>    2.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Energy</th>           <td>    2.4853</td> <td>    0.226</td> <td>   10.997</td> <td> 0.000</td> <td>    2.042</td> <td>    2.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Danceability</th>     <td>   -3.5034</td> <td>    0.255</td> <td>  -13.763</td> <td> 0.000</td> <td>   -4.002</td> <td>   -3.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speechiness</th>      <td>   -4.1893</td> <td>    0.438</td> <td>   -9.574</td> <td> 0.000</td> <td>   -5.047</td> <td>   -3.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Acousticness</th>     <td>   -0.8345</td> <td>    0.150</td> <td>   -5.580</td> <td> 0.000</td> <td>   -1.128</td> <td>   -0.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Instrumentalness</th> <td>    1.6393</td> <td>    0.144</td> <td>   11.392</td> <td> 0.000</td> <td>    1.357</td> <td>    1.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Liveness</th>         <td>    0.3013</td> <td>    0.236</td> <td>    1.278</td> <td> 0.201</td> <td>   -0.161</td> <td>    0.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Valence</th>          <td>   -0.7190</td> <td>    0.179</td> <td>   -4.024</td> <td> 0.000</td> <td>   -1.069</td> <td>   -0.369</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &      harry       & \\textbf{  No. Observations:  } &     5835    \\\\\n",
       "\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &     5827    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        7    \\\\\n",
       "\\textbf{Date:}            & Fri, 02 Aug 2024 & \\textbf{  Pseudo R-squ.:     } &   0.1712    \\\\\n",
       "\\textbf{Time:}            &     14:45:47     & \\textbf{  Log-Likelihood:    } &   -2613.8   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -3153.8   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 5.789e-229  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}        &       2.1849  &        0.213     &    10.273  &         0.000        &        1.768    &        2.602     \\\\\n",
       "\\textbf{Energy}           &       2.4853  &        0.226     &    10.997  &         0.000        &        2.042    &        2.928     \\\\\n",
       "\\textbf{Danceability}     &      -3.5034  &        0.255     &   -13.763  &         0.000        &       -4.002    &       -3.005     \\\\\n",
       "\\textbf{Speechiness}      &      -4.1893  &        0.438     &    -9.574  &         0.000        &       -5.047    &       -3.332     \\\\\n",
       "\\textbf{Acousticness}     &      -0.8345  &        0.150     &    -5.580  &         0.000        &       -1.128    &       -0.541     \\\\\n",
       "\\textbf{Instrumentalness} &       1.6393  &        0.144     &    11.392  &         0.000        &        1.357    &        1.921     \\\\\n",
       "\\textbf{Liveness}         &       0.3013  &        0.236     &     1.278  &         0.201        &       -0.161    &        0.763     \\\\\n",
       "\\textbf{Valence}          &      -0.7190  &        0.179     &    -4.024  &         0.000        &       -1.069    &       -0.369     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  harry   No. Observations:                 5835\n",
       "Model:                          Logit   Df Residuals:                     5827\n",
       "Method:                           MLE   Df Model:                            7\n",
       "Date:                Fri, 02 Aug 2024   Pseudo R-squ.:                  0.1712\n",
       "Time:                        14:45:47   Log-Likelihood:                -2613.8\n",
       "converged:                       True   LL-Null:                       -3153.8\n",
       "Covariance Type:            nonrobust   LLR p-value:                5.789e-229\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept            2.1849      0.213     10.273      0.000       1.768       2.602\n",
       "Energy               2.4853      0.226     10.997      0.000       2.042       2.928\n",
       "Danceability        -3.5034      0.255    -13.763      0.000      -4.002      -3.005\n",
       "Speechiness         -4.1893      0.438     -9.574      0.000      -5.047      -3.332\n",
       "Acousticness        -0.8345      0.150     -5.580      0.000      -1.128      -0.541\n",
       "Instrumentalness     1.6393      0.144     11.392      0.000       1.357       1.921\n",
       "Liveness             0.3013      0.236      1.278      0.201      -0.161       0.763\n",
       "Valence             -0.7190      0.179     -4.024      0.000      -1.069      -0.369\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "model = smf.logit(\"harry ~ Energy + Danceability + Speechiness + Acousticness + Instrumentalness + Liveness + Valence\", data=combined)\n",
    "result = model.fit()\n",
    "with open (\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. That is extremely intimidating and contains a bunch of stuff that requires much more knowledge of statistics to use and explain. But there is one useful gem in here: the `coef` column. These values are the *coefficients* of the logistic regression model that are used to indicate how changes in those features change the output probability that a song belongs to my library vs. E's. First, in order to interpret the coefficients in a slightly more human way, we can convert them to **odds-ratios**, which are quantities that tell us how the output probability changes with a unit change (change of `1`) in that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>odds ratio</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.184899</td>\n",
       "      <td>8.889750</td>\n",
       "      <td>Intercept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.485290</td>\n",
       "      <td>12.004601</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.503449</td>\n",
       "      <td>0.030093</td>\n",
       "      <td>Danceability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.189306</td>\n",
       "      <td>0.015157</td>\n",
       "      <td>Speechiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.834488</td>\n",
       "      <td>0.434097</td>\n",
       "      <td>Acousticness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.639266</td>\n",
       "      <td>5.151388</td>\n",
       "      <td>Instrumentalness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.301303</td>\n",
       "      <td>1.351619</td>\n",
       "      <td>Liveness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.718990</td>\n",
       "      <td>0.487244</td>\n",
       "      <td>Valence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef  odds ratio              name\n",
       "0  2.184899    8.889750         Intercept\n",
       "1  2.485290   12.004601            Energy\n",
       "2 -3.503449    0.030093      Danceability\n",
       "3 -4.189306    0.015157       Speechiness\n",
       "4 -0.834488    0.434097      Acousticness\n",
       "5  1.639266    5.151388  Instrumentalness\n",
       "6  0.301303    1.351619          Liveness\n",
       "7 -0.718990    0.487244           Valence"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefs = pd.DataFrame({\n",
    "    \"coef\": result.params.values,\n",
    "    \"odds ratio\": np.exp(result.params.values),\n",
    "    \"name\": result.params.index\n",
    "})\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to interpret the meaning of the odds ratio, let's imagine that we have a song with the following profile:\n",
    "\n",
    "|Feature|Value|\n",
    "|-------|-----|\n",
    "|Energy|0|\n",
    "|Danceability|0.8|\n",
    "|Speechiness|0.3|\n",
    "|Acousticness|0.2|\n",
    "|Instrumentalness|0.1|\n",
    "|Liveness|0.8|\n",
    "|Valence|0.4|\n",
    "\n",
    "If we use the `predict()` method of the model that we've built, we can observe that the probability that this hypothetical song is a \"Harry song\" is about $12.7\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.127397\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_df = pd.DataFrame([\n",
    "    {'Energy': 0,\n",
    "     'Danceability': 0.8,\n",
    "     'Speechiness': 0.3,\n",
    "     'Acousticness': 0.2,\n",
    "     'Instrumentalness': 0.1,\n",
    "     'Liveness': 0.8,\n",
    "     'Valence': 0.4}\n",
    "])\n",
    "result.predict(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an event is a representation of the likelihood of that event. When expressed as a percentage, it represents how many times out of a hundred that event will be expected to happen. In the context of our prediction model, the percentage refers to the number of songs with exactly this profile that I would be expected to like out of a hundred.\n",
    "\n",
    "Odds are another way of representing an event's likelihood. If something happens with a probability of $x\\%$, then out of $100$ tries, we would expect it to happen $x$ times and therefore for it **not** to happen $100 - x$ times. The odds are expressed as the quotient between the amount of times it would be expected to happen and the amount of times it would not be expected to happen. To give it some specific numbers, if something has a probability of $80\\%$, then out of $100$ tries, it could be expected to happen $80$ times and not happen $20$ times. The odds are therefore expressed as `80:20`, or `4:1` when simplified. *\"For every four successes, we'd expect one failure.\"* The model predicts the song above to have a $12.7\\%$ chance of being from my library instead of E's, or `12.7:87.3` odds, which is very nearly `1:7` (more precisely, `1:6.874`).\n",
    "\n",
    "The **odds-ratio**, then, is a quantity that tells us how much the odds change when the value of a certain variable increases by a value of one. The song above has ~ `1:7` odds of being in my library with the current features—what would the likelihood of the song being mine be if the Energy value was at `1` instead of `0`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.63671\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with_high_energy = pd.DataFrame([\n",
    "    {'Energy': 1,\n",
    "     'Danceability': 0.8,\n",
    "     'Speechiness': 0.3,\n",
    "     'Acousticness': 0.2,\n",
    "     'Instrumentalness': 0.1,\n",
    "     'Liveness': 0.8,\n",
    "     'Valence': 0.4}\n",
    "])\n",
    "result.predict(with_high_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability goes all the way up to $63.7\\%$! That means the odds are now `63.7:36.3`, which is roughly equal to `1.75:1` (or `7:4`, depending on how you want to express that ratio). If we recall from before that the odds-ratio for Energy in our model is `12.0`, then we would expect that increasing Energy by `1`—going from a totally laconic song to an absolutely frenetic one—should increase the odds by a factor of `12`. \n",
    "\n",
    "||Rough Value|Exact Value|\n",
    "|----|----|----|\n",
    "|Old Odds|`1:7`|`1:6.874` $\\approx 0.1459965$|\n",
    "|New Odds|`7:4`|`1.755:1` $\\approx 1.7526219$|\n",
    "\n",
    "Dividing the new odds by the old gives us a quotient of `12.004`, exactly equal to the odds-ratio of the model stated above. \n",
    "\n",
    "### Why Talk About Odds?\n",
    "\n",
    "Logistic regression models are useful for two main reasons: 1) they allow for automatic classification of different datapoints into different categories and 2) they explain how different features ultimately affect the categorization. Sometimes it's enough to build a model that can do classification for you: maybe you want to build a \"priority\" filter for your email so that certain kinds of emails are displayed more prominently in your inbox. In this case, it's not all that important to understand what features of an email (the sender, the time it was sent, the subject line, the number of images contained in it) influence this decision, since you can still see all of the emails you receive anyways. Other models for more serious tasks, like those currently used for approving and denying loans or weighing an incarcerated person's risk of recidivism, raise serious questions about how they make their decisions. Moreover, from an investigative sense, it is often vital to look beyond the outcomes and try to understand the effects that different \"variables\" have on the systems we use. How does a person's age, gender, race, level of education, or even zip code get used to make decisions about whether they can take out a mortgage or be released from prison on parole?\n",
    "\n",
    "The odds-ratio in a logistic regression model is useful for exactly these purposes. Given the model, we can analyze the characteristics of a data point that lead to the decision being made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
